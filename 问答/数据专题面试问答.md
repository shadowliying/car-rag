# 数据专题面试问答

> 本文档覆盖RAG系统中的数据构造、标注、评测等核心问题，基于智能座舱汽车知识问答系统项目实践整理。

---

## 一、测试数据构造

### Q1：你们项目的测试集是怎么构造的？

**回答：**

我们测试集构造分三步走：

**1. 种子问题收集（600条）**
- 从业务方收集真实用户问题
- 产品经理根据文档内容设计典型问题
- 我自己阅读文档后补充边界case

**2. GPT-4增强扩写（600→2000条）**
```
Prompt设计：
"基于以下问题，生成3个语义相似但表述不同的问题变体：
- 原问题：{question}
- 要求：保持问题意图一致，变换疑问词、句式、口语化程度"
```

**3. 人工校验与标注**
- 检查扩写问题是否合理
- 标注每个问题对应的正确答案段落
- 剔除歧义或无法回答的问题

最终得到2000条测试样本，覆盖了智能座舱的各个功能模块。

---

### Q2：为什么选择GPT-4做数据增强而不是其他方法？

**回答：**

对比了几种方案：

| 方法 | 优点 | 缺点 |
|------|------|------|
| 规则替换 | 快速、可控 | 多样性差，容易生成不自然的句子 |
| 回译 | 能产生不同表述 | 可能改变语义，中文效果一般 |
| GPT-4生成 | 多样性好，语义保持准确 | 成本较高 |
| 小模型生成 | 成本低 | 质量不稳定 |

选择GPT-4的原因：
1. **语义保持好**：能准确理解问题意图，生成的变体不会偏离原意
2. **多样性强**：可以生成口语化、书面化、简洁、详细等不同风格
3. **可控性**：通过Prompt可以精确控制生成方向
4. **效率**：一次调用生成多个变体，整体成本可接受

---

### Q3：测试集的质量如何保证？

**回答：**

三重保障机制：

**1. 分层抽样检查**
- 每个功能模块至少抽查20%
- 重点检查扩写的问题是否合理

**2. 交叉验证**
- 两人独立标注相同问题
- 计算标注一致性（Kappa系数>0.8才通过）
- 不一致的样本由第三人仲裁

**3. 答案可追溯**
- 每个问题必须能在文档中找到对应段落
- 标注时记录答案来源的chunk_id

最终通过这套流程，测试集的标注准确率达到95%以上。

---

## 二、训练数据构造

### Q4：Embedding模型的训练数据是怎么构造的？

**回答：**

我们构造了4200条三元组数据用于Embedding微调：

**数据格式：(Query, Positive, Negative)**
- Query：用户问题
- Positive：正确答案段落
- Negative：相关但不正确的段落（Hard Negative）

**构造流程：**

```python
# 1. 基于测试集的正样本
for question, answer_chunk in test_data:
    positive = answer_chunk
    
    # 2. Hard Negative Mining
    # 用基础M3E召回Top20，排除正确答案后的段落作为负样本
    candidates = base_m3e.retrieve(question, top_k=20)
    hard_negatives = [c for c in candidates if c.id != answer_chunk.id][:3]
    
    # 3. 生成三元组
    for neg in hard_negatives:
        triplets.append((question, positive, neg))
```

**数据比例：**
- 600条种子问题 × 7条三元组/问题 ≈ 4200条
- 正负样本比例 1:3（每个正样本配3个负样本）

---

### Q5：什么是Hard Negative Mining？为什么重要？

**回答：**

**定义：**
Hard Negative是指与Query语义相关、容易被模型混淆、但实际上不是正确答案的样本。

**对比：**
```
Query: "智能座舱支持哪些语音指令？"

Easy Negative: "汽车保养周期是多久？"     （完全不相关）
Hard Negative: "智能座舱的语音识别准确率"  （相关但不是答案）
```

**为什么重要：**
1. **Easy Negative太简单**：模型很容易区分，学不到有用信息
2. **Hard Negative提供更强监督信号**：迫使模型学习细粒度的语义差异
3. **直接提升召回质量**：减少"相关但不正确"的召回结果

**我们的挖掘策略：**
- 用当前模型召回Top-K
- 排除正确答案后，取相似度较高的作为Hard Negative
- 这种"自举"方式效果最好

---

### Q6：Rerank模型的训练数据怎么构造？

**回答：**

Rerank训练数据共600条，格式为二分类：

```python
# 数据格式
{
    "query": "问题",
    "passage": "段落",
    "label": 1 或 0  # 1表示相关，0表示不相关
}
```

**构造策略：**

**正样本（label=1）：**
- 直接使用测试集中问题对应的正确答案段落
- 600条问题 × 1条正样本 = 600条

**负样本（label=0）：**
- 从Embedding召回结果中选取非正确答案的段落
- 600条问题 × 2条负样本 = 1200条

**正负比例：1:2**

为什么不用1:1？
- 实际场景中，召回结果大部分是负样本
- 1:2更接近真实分布，模型泛化性更好

---

### Q7：训练数据量够吗？怎么判断是否需要更多数据？

**回答：**

**判断数据量是否充足的方法：**

**1. 学习曲线分析**
```python
# 用不同数据量训练，观察验证集指标
data_sizes = [1000, 2000, 3000, 4200]
for size in data_sizes:
    model = train(data[:size])
    score = evaluate(model, val_set)
    # 如果曲线趋于平缓，说明数据量够了
```

**2. 过拟合观察**
- 训练Loss持续下降，验证Loss开始上升 → 数据不够
- 两条曲线趋势一致 → 数据量OK

**我们的情况：**
- Embedding微调：4200条三元组，Recall@10提升3%，曲线已平缓
- Rerank微调：600条样本，Precision提升2%，效果明显

**如果需要更多数据的策略：**
1. 扩展种子问题（成本最高但质量最好）
2. 更多的Hard Negative采样
3. 数据增强（同义词替换、回译等）

---

## 三、评测方法

### Q8：召回阶段用什么指标评估？

**回答：**

主要使用三个指标：

**1. Hit Rate@K（命中率）**
```python
def hit_rate(queries, ground_truth, k=10):
    hits = 0
    for q in queries:
        retrieved = retriever.search(q, top_k=k)
        if ground_truth[q] in retrieved:
            hits += 1
    return hits / len(queries)
```
含义：Top-K结果中包含正确答案的比例

**2. MRR（Mean Reciprocal Rank）**
```python
def mrr(queries, ground_truth):
    rr_sum = 0
    for q in queries:
        retrieved = retriever.search(q, top_k=100)
        for i, doc in enumerate(retrieved):
            if doc == ground_truth[q]:
                rr_sum += 1 / (i + 1)
                break
    return rr_sum / len(queries)
```
含义：正确答案排名的倒数的平均值，越高越好

**3. Recall@K**
```python
# 当每个Query有多个相关文档时
def recall_at_k(queries, ground_truth_list, k=10):
    recall_sum = 0
    for q in queries:
        retrieved = set(retriever.search(q, top_k=k))
        relevant = set(ground_truth_list[q])
        recall_sum += len(retrieved & relevant) / len(relevant)
    return recall_sum / len(queries)
```

**我们的结果：**
- Embedding微调后：Recall@10 从85%提升到88%（+3%）
- 测试集：400条（从2000条中分出专门评估Embedding的子集）

---

### Q9：Rerank阶段用什么指标评估？

**回答：**

**主要指标：**

**1. Precision@K**
```python
def precision_at_k(queries, ground_truth, k=3):
    precision_sum = 0
    for q in queries:
        reranked = reranker.rerank(q, candidates, top_k=k)
        relevant = sum(1 for doc in reranked if doc in ground_truth[q])
        precision_sum += relevant / k
    return precision_sum / len(queries)
```
含义：Rerank后Top-K中正确答案的比例

**2. NDCG@K（归一化折损累积增益）**
- 考虑了排序位置的影响
- 正确答案排在前面得分更高

**3. 提升率（对比Rerank前后）**
```python
before = precision_without_rerank(candidates)
after = precision_with_rerank(candidates)
improvement = (after - before) / before * 100
```

**我们的结果：**
- Rerank后：Precision@3 提升2%
- 说明Rerank有效地把正确答案往前排了

---

### Q10：端到端的评估怎么做？

**回答：**

端到端评估是对整个RAG系统的综合评估：

**评估流程：**
```
用户问题 → 召回 → Rerank → LLM生成 → 评估答案质量
```

**评估指标：**

**1. 准确率（Accuracy）**
- 人工判断生成的答案是否正确回答了问题
- 我们项目：2000条测试样本，准确率88.6%

**2. 答案相关性（Relevance）**
- 1-5分评分
- 5分：完全正确且全面
- 3分：部分正确
- 1分：完全不相关

**3. 忠实度（Faithfulness）**
- 答案是否基于召回的文档，而非模型幻觉
- 检查答案中的关键信息是否能在参考文档中找到

**自动化评估尝试：**
```python
# 用GPT-4做自动评估
prompt = f"""
问题：{question}
标准答案：{golden_answer}
模型答案：{model_answer}

请判断模型答案是否正确，输出：正确/错误/部分正确
"""
```

但最终还是以人工评估为准，自动评估仅作参考。

---

### Q11：评测中遇到过什么问题？怎么解决的？

**回答：**

**问题1：标注一致性差**
- 现象：不同人对同一答案的评分差异大
- 解决：制定详细的评分标准，附带示例；不一致的由第三人仲裁

**问题2：部分正确难以界定**
- 现象：答案包含正确信息但不完整
- 解决：引入细粒度评分（1-5分），而非简单的对/错

**问题3：评估效率低**
- 现象：2000条人工评估耗时太长
- 解决：
  - 先用自动指标筛选，只人工评估自动评估"不确定"的样本
  - 分批评估，每批200条
  - 抽样评估（在保证覆盖率的前提下）

**问题4：测试集泄露**
- 现象：担心测试数据被用于训练
- 解决：严格隔离测试集，训练数据和测试数据无重叠

---

## 四、数据增强与优化

### Q12：你们用了哪些数据增强方法？

**回答：**

针对不同场景使用不同的增强策略：

**1. 问题扩写（GPT-4）**
```python
prompt = """
原问题：智能座舱怎么连接手机？
请生成3个语义相同但表述不同的问题：
1. 口语化版本
2. 书面化版本
3. 简洁版本
"""
# 输出：
# 1. 咋用智能座舱连手机啊？
# 2. 请问智能座舱与手机的连接方式是什么？
# 3. 座舱连手机步骤？
```

**2. 同义词替换**
```python
# 基于词典的替换
synonyms = {
    "智能座舱": ["智能驾驶舱", "车载智能系统"],
    "连接": ["链接", "配对", "对接"]
}
```

**3. 实体替换**
```python
# 保持句式，替换具体实体
"Model X的续航里程是多少？" → "Model Y的续航里程是多少？"
```

**4. 句式变换**
- 陈述句 ↔ 疑问句
- 主动句 ↔ 被动句

**效果：**
- 通过增强，种子问题从600扩展到2000
- 模型泛化性明显提升

---

### Q13：数据清洗做了哪些工作？

**回答：**

**1. 文档预处理**
```python
def clean_document(text):
    # 去除特殊字符
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # 统一空白符
    text = re.sub(r'\s+', ' ', text)
    # 去除页眉页脚（PDF解析残留）
    text = remove_headers_footers(text)
    return text.strip()
```

**2. 问题清洗**
```python
def clean_question(q):
    # 去除前后空白
    q = q.strip()
    # 统一标点
    q = q.replace('？', '?').replace('，', ',')
    # 去除重复问号
    q = re.sub(r'\?+', '?', q)
    return q
```

**3. 去重**
```python
# 基于编辑距离去重
def deduplicate(questions, threshold=0.9):
    unique = []
    for q in questions:
        is_dup = False
        for u in unique:
            if edit_distance_ratio(q, u) > threshold:
                is_dup = True
                break
        if not is_dup:
            unique.append(q)
    return unique
```

**4. 格式校验**
- 问题必须以问号或疑问词结尾
- 答案长度在合理范围内（50-500字）

---

### Q14：如何处理数据不平衡问题？

**回答：**

我们的数据存在两类不平衡：

**1. 类别不平衡（正负样本）**
```
问题：Embedding训练中，正样本远少于负样本
解决：
- 控制采样比例（1:3）
- 使用对比学习Loss（InfoNCE），天然处理多负样本
```

**2. 领域不平衡（不同功能模块）**
```
问题：语音控制相关问题多，自动泊车问题少
解决：
- 对少数类别进行过采样（重复采样）
- 对少数类别做更多数据增强
- 评估时分模块统计，确保每个模块都达标
```

**效果验证：**
- 处理前：自动泊车模块准确率仅82%
- 处理后：准确率提升到87%，与其他模块持平

---

## 五、数据质量与问题

### Q15：数据标注过程中遇到的最大挑战是什么？

**回答：**

**最大挑战：歧义问题的处理**

**案例：**
```
问题："智能座舱的屏幕尺寸多大？"
文档中有：
- 中控屏：15.6英寸
- 仪表屏：12.3英寸  
- 副驾屏：8英寸
```

这种情况，答案应该是哪个？

**解决方案：**

1. **建立标注规范**
   - 歧义问题标注为"多答案"类型
   - 召回时要求命中任一相关段落即可
   - 生成时要求列举所有相关信息

2. **问题改写**
   - 将歧义问题改写为明确问题
   - "智能座舱的屏幕尺寸多大？" → "智能座舱中控屏的尺寸是多少？"

3. **引入澄清机制**
   - 检测到歧义问题时，先反问用户
   - "您是想了解中控屏、仪表屏还是副驾屏的尺寸？"

---

### Q16：如何验证训练数据的质量？

**回答：**

**三层验证机制：**

**1. 统计检验**
```python
def data_statistics(dataset):
    print(f"总样本数: {len(dataset)}")
    print(f"问题平均长度: {avg([len(d['query']) for d in dataset])}")
    print(f"正负样本比: {count_pos/count_neg}")
    print(f"类别分布: {Counter([d['category'] for d in dataset])}")
```
检查是否有异常分布

**2. 抽样人工检查**
```python
# 随机抽取5%样本人工复核
sample = random.sample(dataset, int(len(dataset) * 0.05))
for item in sample:
    # 检查query和positive是否真的相关
    # 检查negative是否真的不相关
```

**3. 训练反馈验证**
- 如果模型在验证集上效果差，可能是训练数据质量问题
- 分析Bad Case，追溯是否因为训练数据标注错误

**质量指标：**
- 标注一致性 > 90%（Kappa系数）
- 抽样复核正确率 > 95%

---

## 六、面试高频问题

### Q17：如果让你重新做数据，你会改进什么？

**回答：**

**1. 更早引入主动学习**
```
现状：先标注完所有数据再训练
改进：
- 标注一批 → 训练 → 找出模型不确定的样本 → 优先标注这些
- 可以用更少的标注量达到相同效果
```

**2. 引入用户反馈循环**
```
现状：测试集是静态的
改进：
- 上线后收集用户反馈（点赞/踩）
- 将用户标记的Bad Case加入测试集
- 持续优化
```

**3. 更细粒度的负样本分级**
```
现状：负样本只分Hard/Easy两类
改进：
- 引入Semi-Hard Negative
- 根据相似度分数划分难度等级
- 课程学习：先学Easy，再学Semi-Hard，最后Hard
```

---

### Q18：你们的数据有没有做版本管理？

**回答：**

**有，使用Git + DVC（Data Version Control）：**

```bash
# 目录结构
data/
├── raw/              # 原始数据
├── processed/        # 处理后的数据
├── train/            # 训练集
├── test/             # 测试集
└── .dvc/             # DVC版本控制
```

**版本管理策略：**

1. **数据文件用DVC追踪**
```bash
dvc add data/train/embedding_triplets.json
git add data/train/embedding_triplets.json.dvc
git commit -m "Add embedding training data v1.0"
```

2. **配套数据说明文档**
```markdown
## v1.0 (2024-01-15)
- 样本数：4200条
- 来源：600种子问题 + Hard Negative Mining
- 变更：初始版本

## v1.1 (2024-02-01)
- 样本数：4500条
- 变更：新增300条边界case
```

3. **与模型版本关联**
- 每个模型checkpoint记录使用的数据版本
- 方便复现和回滚

---

### Q19：怎么处理数据隐私和安全问题？

**回答：**

**1. 数据脱敏**
```python
def desensitize(text):
    # 手机号脱敏
    text = re.sub(r'1[3-9]\d{9}', '1XX****XXXX', text)
    # 身份证脱敏
    text = re.sub(r'\d{17}[\dXx]', '***', text)
    # 姓名脱敏（如果有标注）
    text = mask_names(text)
    return text
```

**2. 访问控制**
- 测试数据存储在内网服务器
- 只有项目组成员有访问权限
- 下载需要审批

**3. 使用规范**
- 禁止将测试数据用于其他项目
- 禁止上传到公开平台
- 定期审计数据访问日志

---

### Q20：如果数据量不够，有什么低成本扩充方案？

**回答：**

**成本从低到高的方案：**

**1. 自动化增强（零成本）**
```python
# 同义词替换
# 随机删除/插入
# 句子重排
from nlpaug import augmenter as naw
aug = naw.SynonymAug()
augmented = aug.augment(text)
```

**2. 半监督学习（低成本）**
```python
# 用已训练模型预测未标注数据
# 选择高置信度样本作为伪标签
for unlabeled in unlabeled_data:
    pred, conf = model.predict(unlabeled)
    if conf > 0.95:
        training_data.append((unlabeled, pred))
```

**3. 众包标注（中等成本）**
- 使用平台如Amazon Mechanical Turk
- 设计简单的标注任务
- 多人标注取共识

**4. Few-shot Prompt（适合快速验证）**
```python
prompt = """
以下是一些问答示例：
Q: xxx  A: xxx
Q: xxx  A: xxx

请为以下文档生成5个问答对：
{document}
"""
```

**我们的经验：**
方案1+4的组合效果最好，成本低且质量可控。

---

## 总结

| 数据类型 | 数量 | 用途 | 关键点 |
|---------|------|------|--------|
| 测试集 | 2000条 | 端到端评估 | GPT-4扩写 + 人工校验 |
| Embedding训练 | 4200条 | 微调M3E | Hard Negative Mining |
| Rerank训练 | 600条 | 微调BGE | 1:2正负比例 |
| 验证集 | 400条 | Embedding评估 | 从测试集分出 |

**核心原则：**
1. 数据质量 > 数据数量
2. Hard Negative是提升召回质量的关键
3. 评估指标要与业务目标对齐
4. 版本管理和可追溯性很重要

---

## 附录：数据全流程详解

> 本节完整梳理从原始文档到微调数据的全流程，帮助理清各数据集之间的关系。

### 一、原始数据来源

```
┌─────────────────────────────────────────────────────────────┐
│                    智能座舱产品文档（PDF）                    │
│                     约50份，共500页                          │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    PDF解析 + 分块                            │
│             PyMuPDF提取 → 按段落/语义切分                     │
│               每块200-500字，保持语义完整                     │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
                    知识库：约10000个Chunks
                     （这是向量检索的底库）
```

---

### 二、问答数据构造

```
┌─────────────────────────────────────────────────────────────┐
│                 种子问题收集（600条）                         │
│  来源：                                                      │
│  - 业务方提供的真实用户问题                                   │
│  - 产品经理根据文档设计的典型问题                             │
│  - 自己阅读文档后补充的边界case                               │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                 GPT-4扩写（600 → 2000条）                    │
│  Prompt: "生成3个语义相同但表述不同的问题变体"                │
│  - 口语化、书面化、简洁版等不同风格                           │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                 人工标注（2000条）                            │
│  - 为每个问题标注对应的正确答案chunk_id                       │
│  - 剔除歧义/无法回答的问题                                    │
│  - 最终得到：(问题, 正确答案chunk_id) 的配对                  │
└─────────────────────────────────────────────────────────────┘
```

---

### 三、数据集划分

```
              2000条问答对（带标注）
                      │
      ┌───────────────┼───────────────┐
      │               │               │
      ▼               ▼               ▼
  训练集           验证集           测试集
  1600条           200条            200条
   (80%)           (10%)            (10%)
      │               │               │
      │               │               │
      ▼               ▼               ▼
 构造微调数据    训练时验证指标    最终效果评估
                 (早停/调参)      (88.6%准确率)
```

**关键原则：测试集200条完全隔离，从不参与任何训练过程！**

---

### 四、Embedding微调数据构造

```
输入：训练集1600条 (问题, 正确答案chunk_id)
输出：约4200条三元组 (Query, Positive, Negative)

┌─────────────────────────────────────────────────────────────┐
│                    构造流程                                  │
│                                                             │
│  for question, answer_chunk_id in 训练集[:1400]:            │
│      │                                                      │
│      │  Step1: 获取正样本                                   │
│      │  positive = knowledge_base[answer_chunk_id]          │
│      │                                                      │
│      │  Step2: Hard Negative Mining                         │
│      │  candidates = base_M3E.retrieve(question, top_k=20)  │
│      │  # 用基础M3E召回Top20                                 │
│      │  # 排除正确答案，取相似度高的作为Hard Negative        │
│      │  hard_negatives = 筛选出2-3个                        │
│      │                                                      │
│      │  Step3: 生成三元组                                   │
│      │  for neg in hard_negatives:                          │
│      │      triplets.append((question, positive, neg))      │
│      ▼                                                      │
│                                                             │
│  结果：1400条 × 3个负样本 ≈ 4200条三元组                     │
└─────────────────────────────────────────────────────────────┘

数据格式：
{
    "query": "智能座舱怎么连蓝牙？",
    "positive": "蓝牙连接步骤：1.打开设置...",
    "negative": "蓝牙版本支持5.0协议..."  ← Hard Negative（相关但不是答案）
}

训练方式：对比学习（InfoNCE Loss）
验证指标：用验证集200条计算 Recall@10
```

---

### 五、Rerank微调数据构造

```
输入：训练集中取600条 (问题, 正确答案chunk_id)
输出：1800条二分类数据

┌─────────────────────────────────────────────────────────────┐
│                    构造流程                                  │
│                                                             │
│  for question, answer_chunk_id in 训练集[:600]:             │
│      │                                                      │
│      │  Step1: 正样本 (label=1)                             │
│      │  data.append({                                       │
│      │      "query": question,                              │
│      │      "passage": knowledge_base[answer_chunk_id],     │
│      │      "label": 1                                      │
│      │  })                                                  │
│      │                                                      │
│      │  Step2: 负样本 (label=0)                             │
│      │  # 从Embedding召回结果中选取非正确答案的段落           │
│      │  candidates = embedding_model.retrieve(question)     │
│      │  negatives = 选2个非正确答案的段落                    │
│      │  for neg in negatives:                               │
│      │      data.append({                                   │
│      │          "query": question,                          │
│      │          "passage": neg,                             │
│      │          "label": 0                                  │
│      │      })                                              │
│      ▼                                                      │
│                                                             │
│  结果：600正样本 + 1200负样本 = 1800条                       │
│  正负比例：1:2                                               │
└─────────────────────────────────────────────────────────────┘

数据格式：
{
    "query": "智能座舱怎么连蓝牙？",
    "passage": "蓝牙连接步骤：1.打开设置...",
    "label": 1  // 相关
}

训练方式：二分类（Cross-entropy Loss）
验证指标：用验证集200条计算 Precision@3
```

---

### 六、各数据集用途总结

| 数据集 | 数量 | 来源 | 格式 | 用途 |
|--------|------|------|------|------|
| **知识库Chunks** | ~10000条 | PDF解析分块 | 文本段落 | 向量检索的底库 |
| **问答对总集** | 2000条 | 种子+GPT-4扩写+人工标注 | (Query, Answer_ID) | 划分为下面三个 |
| **训练集** | 1600条 | 问答对的80% | (Query, Answer_ID) | 构造微调数据 |
| **验证集** | 200条 | 问答对的10% | (Query, Answer_ID) | 训练时验证/早停 |
| **测试集** | 200条 | 问答对的10% | (Query, Answer_ID) | 最终效果评估 |
| **Embedding三元组** | ~4200条 | 从训练集构造 | (Q, Pos, Neg) | 微调M3E |
| **Rerank二分类** | 1800条 | 从训练集构造 | (Q, P, Label) | 微调BGE-reranker |

---

### 七、验证流程

```
┌─────────────────────────────────────────────────────────────┐
│                    Embedding微调验证                         │
│                                                             │
│  每个epoch结束后：                                           │
│  for q, answer_id in 验证集200条:                            │
│      retrieved = M3E.retrieve(q, top_k=10)                  │
│      if answer_id in retrieved:                             │
│          hit += 1                                           │
│  Recall@10 = hit / 200                                      │
│                                                             │
│  结果：85% → 88%（+3%）                                      │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    Rerank微调验证                            │
│                                                             │
│  每个epoch结束后：                                           │
│  for q, answer_id in 验证集200条:                            │
│      candidates = embedding.retrieve(q, top_k=20)           │
│      reranked = reranker.rerank(q, candidates, top_k=3)     │
│      if answer_id in reranked[:3]:                          │
│          hit += 1                                           │
│  Precision@3 = hit / 200                                    │
│                                                             │
│  结果：提升2%                                                │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    端到端测试（最终）                         │
│                                                             │
│  用测试集200条：                                             │
│  for q, golden_answer in 测试集200条:                        │
│      # 完整RAG流程                                          │
│      retrieved = embedding.retrieve(q)                      │
│      reranked = reranker.rerank(q, retrieved)               │
│      answer = LLM.generate(q, reranked)                     │
│      # 人工判断                                              │
│      if 人工评估(answer, golden_answer) == 正确:             │
│          correct += 1                                       │
│  Accuracy = correct / 200                                   │
│                                                             │
│  结果：88.6%                                                 │
└─────────────────────────────────────────────────────────────┘
```

---

### 八、一句话总结

> **50份PDF → 解析成10000个Chunks作为知识库 → 人工设计600种子问题 → GPT-4扩写到2000条 → 人工标注答案 → 划分为1600训练+200验证+200测试 → 从训练集构造4200条三元组微调Embedding、1800条样本微调Rerank → 最终在测试集上达到88.6%准确率**
