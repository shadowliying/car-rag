# 基础八股面试问答

> 本文档覆盖RAG项目面试中常问的基础知识，包括Transformer、BERT/GPT、Embedding、对比学习、大模型、向量检索、损失函数等。

---

## 一、Transformer 架构

### Q1：Transformer的整体架构是什么？

**直觉理解：**
Transformer就像一个"翻译机器"，原版是用来做机器翻译的（英→中）。

**架构图：**
```
输入序列 "I love you"
        │
        ▼
┌───────────────────┐
│     Encoder       │  ← 理解输入（双向看）
│  (6层堆叠)         │
└───────────────────┘
        │
        ▼ (编码后的表示)
┌───────────────────┐
│     Decoder       │  ← 生成输出（单向看）
│  (6层堆叠)         │
└───────────────────┘
        │
        ▼
输出序列 "我 爱 你"
```

**每层的组成：**
```
Encoder层:
Input → Self-Attention → Add & Norm → FFN → Add & Norm → Output

Decoder层:
Input → Masked Self-Attention → Add & Norm 
      → Cross-Attention (看Encoder) → Add & Norm 
      → FFN → Add & Norm → Output
```

**核心组件：**
1. **Self-Attention**：让每个词看到其他所有词
2. **Feed Forward Network (FFN)**：两层全连接，做非线性变换
3. **Add & Norm**：残差连接 + Layer Normalization

---

### Q2：Self-Attention是怎么计算的？

**直觉理解：**
每个词都去"问"其他词："你和我有多相关？"，然后根据相关程度加权聚合信息。

**计算步骤：**

```
输入: X = [x1, x2, x3]  （3个词的向量）

第1步：生成Q、K、V
Q = X × Wq  （Query：我要查什么）
K = X × Wk  （Key：我有什么）
V = X × Wv  （Value：我的内容是什么）

第2步：计算注意力分数
Score = Q × K^T  （点积，衡量相关性）

第3步：缩放
Score = Score / √dk  （dk是K的维度）

第4步：Softmax归一化
Attention = Softmax(Score)  （变成概率分布）

第5步：加权求和
Output = Attention × V
```

**公式（一行）：**
$$\text{Attention}(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**具体例子：**
```python
# 假设输入3个词，每个词8维
X = torch.randn(3, 8)  # [seq_len, d_model]

# 权重矩阵
Wq = torch.randn(8, 8)
Wk = torch.randn(8, 8)
Wv = torch.randn(8, 8)

# 计算Q、K、V
Q = X @ Wq  # [3, 8]
K = X @ Wk  # [3, 8]
V = X @ Wv  # [3, 8]

# 注意力分数
scores = Q @ K.T  # [3, 3] 每个词对每个词的分数
scores = scores / (8 ** 0.5)  # 缩放

# Softmax
attn = torch.softmax(scores, dim=-1)  # [3, 3]

# 输出
output = attn @ V  # [3, 8]
```

---

### Q3：为什么要除以√dk？

**问题：** 如果不除，会怎样？

**原因：**
- Q和K做点积，结果的方差会随着维度dk增大而增大
- 假设Q和K的每个元素都是均值0、方差1的分布
- 点积结果的方差 = dk（维度越大，值越大）

**后果：**
- 值太大 → Softmax后梯度消失
- 比如 Softmax([100, 1, 1]) ≈ [1, 0, 0]，梯度几乎为0

**解决：**
- 除以√dk，让方差回到1
- Softmax输入在合理范围，梯度正常

```python
# 不缩放
scores = [100, 50, 50]
softmax(scores) = [1.0, 0.0, 0.0]  # 梯度消失！

# 缩放后（假设dk=100，除以10）
scores = [10, 5, 5]
softmax(scores) = [0.95, 0.025, 0.025]  # 有梯度
```

---

### Q4：Multi-Head Attention有什么好处？

**直觉理解：**
一个头只能关注一种关系，多个头可以同时关注多种关系。

**例子：**
```
句子："小明把苹果给了小红"

Head 1 可能关注：主谓关系（小明→给）
Head 2 可能关注：动宾关系（给→苹果）
Head 3 可能关注：间接宾语（给→小红）
```

**计算过程：**
```python
# 假设 d_model=512, num_heads=8
# 每个头的维度 d_k = 512/8 = 64

# 并行计算8个头
head_1 = Attention(Q1, K1, V1)  # [seq_len, 64]
head_2 = Attention(Q2, K2, V2)  # [seq_len, 64]
...
head_8 = Attention(Q8, K8, V8)  # [seq_len, 64]

# 拼接
concat = [head_1; head_2; ...; head_8]  # [seq_len, 512]

# 最后一个线性变换
output = concat @ Wo  # [seq_len, 512]
```

**好处：**
1. **多角度理解**：不同头捕捉不同类型的关系
2. **计算效率**：并行计算，总计算量不变
3. **表达能力强**：相当于集成多个子空间的注意力

---

### Q5：Position Encoding为什么用sin/cos？

**问题：** Self-Attention是无序的！

```python
# 这两个句子，对Attention来说一样
"我 爱 你" 
"你 爱 我"
# 因为Attention只看词之间的关系，不看位置
```

**解决：** 加上位置编码

**sin/cos公式：**
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

- pos：位置（0, 1, 2, ...）
- i：维度（0, 1, 2, ...）

**为什么用sin/cos？**

1. **值域有界**：[-1, 1]，不会太大
2. **可以表示相对位置**：
   - PE(pos+k) 可以由 PE(pos) 线性变换得到
   - 因为 sin(a+b) = sin(a)cos(b) + cos(a)sin(b)
3. **可推广到任意长度**：训练时没见过的位置也能编码

**可视化：**
```
位置0: [sin(0), cos(0), sin(0), cos(0), ...]
位置1: [sin(1), cos(1), sin(0.1), cos(0.1), ...]
位置2: [sin(2), cos(2), sin(0.2), cos(0.2), ...]

低维：变化快（捕捉近距离位置差异）
高维：变化慢（捕捉远距离位置差异）
```

---

### Q6：Layer Norm vs Batch Norm区别？

**Batch Norm（批归一化）：**
```
在Batch维度上归一化

输入: [Batch, Seq, Feature]
      [  B  ,  S ,    F   ]
           ↓
对每个Feature，在所有Batch的所有位置上算均值和方差
```

**Layer Norm（层归一化）：**
```
在Feature维度上归一化

输入: [Batch, Seq, Feature]
      [  B  ,  S ,    F   ]
           ↓
对每个样本的每个位置，在所有Feature上算均值和方差
```

**图示：**
```
        Feature →
Batch ┌─────────────┐
  ↓   │ ■ ■ ■ ■ ■ ■ │  Batch Norm: 竖着算（同一列）
      │ ■ ■ ■ ■ ■ ■ │
      │ ■ ■ ■ ■ ■ ■ │  Layer Norm: 横着算（同一行）
      └─────────────┘
```

**为什么Transformer用Layer Norm？**
1. **序列长度可变**：Batch Norm要求同一位置，但句子长度不同
2. **训练推理一致**：Layer Norm不依赖batch统计量
3. **实践效果好**：NLP任务上Layer Norm更稳定

---

### Q7：Feed Forward层的作用？

**结构：**
```python
FFN(x) = ReLU(x @ W1 + b1) @ W2 + b2

# 维度变化
# x: [seq_len, 512]
# W1: [512, 2048]  # 先扩大4倍
# W2: [2048, 512]  # 再缩回来
```

**作用：**

1. **增加非线性**：Attention是线性的（加权求和），FFN加入ReLU
2. **增加表达能力**：先升维再降维，相当于压缩-解压
3. **逐位置处理**：每个位置独立过FFN，不同位置可以学到不同变换

**类比：**
- Attention：全局信息交换（看其他词）
- FFN：局部特征提取（处理自己）

---

### Q8：Transformer相比RNN的优势？

| 方面 | RNN | Transformer |
|------|-----|-------------|
| **并行性** | ❌ 必须顺序计算 | ✅ 可以并行 |
| **长距离依赖** | ❌ 梯度消失/爆炸 | ✅ 直接Attention |
| **计算复杂度** | O(n) | O(n²) 但GPU友好 |
| **训练效率** | 慢（串行） | 快（并行） |

**RNN的问题：**
```
句子: "我 昨天 在 北京 吃 了 一碗 非常 好吃 的 牛肉面"

RNN处理：我 → 昨天 → 在 → ... → 牛肉面
         信息一层层传递，"我"的信息到"牛肉面"已经衰减

Transformer处理：
每个词直接Attention到所有词，"我"可以直接看到"牛肉面"
```

---

## 二、BERT vs GPT

### Q9：BERT和GPT的核心区别？

| 方面 | BERT | GPT |
|------|------|-----|
| **架构** | Encoder-only | Decoder-only |
| **注意力** | 双向（看左看右） | 单向（只看左边） |
| **预训练任务** | MLM + NSP | CLM（预测下一个词） |
| **适合任务** | 理解类（分类、NER） | 生成类（对话、续写） |

**图示：**
```
BERT (双向):
"我 [MASK] 苹果"
 ↓    ↓    ↓
每个词可以看到所有词
[MASK]位置：左看"我"，右看"苹果" → 预测"吃"

GPT (单向):
"我 吃"
 ↓  ↓
每个词只能看到左边的词
"吃"位置：只能看"我" → 预测下一个词"苹果"
```

---

### Q10：什么是Masked Language Model（MLM）？

**BERT的预训练任务之一**

**做法：**
1. 随机遮住15%的词
2. 让模型预测被遮住的词

```
原句：    "我 喜欢 吃 苹果 和 香蕉"
遮住后：  "我 喜欢 [MASK] 苹果 和 [MASK]"
预测：    [MASK1] → "吃"，[MASK2] → "香蕉"
```

**15%中的细分：**
- 80%：替换成[MASK]
- 10%：替换成随机词
- 10%：保持不变

**为什么这样设计？**
- 如果100%用[MASK]，模型只学会处理[MASK]
- 加入随机词和不变，让模型学会处理正常句子

---

### Q11：什么是Causal Language Model（CLM）？

**GPT的预训练任务**

**做法：**
根据前面的词，预测下一个词

```
输入：  "今天 天气"
目标：  预测下一个词 "很好"

训练时：
"今天" → 预测 "天气"
"今天 天气" → 预测 "很好"
"今天 天气 很好" → 预测 "适合"
...
```

**Causal的含义：**
- 因果关系：后面的词由前面的词决定
- 单向注意力：Mask掉右边的词

```python
# Causal Attention Mask
mask = [
    [1, 0, 0, 0],  # 位置0只能看位置0
    [1, 1, 0, 0],  # 位置1能看0,1
    [1, 1, 1, 0],  # 位置2能看0,1,2
    [1, 1, 1, 1],  # 位置3能看0,1,2,3
]
```

---

### Q12：BERT为什么适合做Embedding？

**原因1：双向理解**
```
句子："苹果发布了新手机"

BERT：
"苹果"可以看到"手机" → 知道这是公司Apple
"苹果"可以看到"发布" → 知道是动作的主语

GPT：
"苹果"只能看到前面（没有前面） → 不知道是水果还是公司
```

**原因2：预训练任务**
- MLM要求理解上下文 → 学到的表示更有语义信息
- 适合做语义相似度、分类等理解任务

**原因3：[CLS] token**
```
输入：[CLS] 我 喜欢 苹果 [SEP]
输出：[h_CLS, h_我, h_喜欢, h_苹果, h_SEP]

h_CLS 聚合了整个句子的信息，可以作为句子Embedding
```

---

### Q13：GPT为什么适合做生成？

**原因1：单向设计天然适合生成**
```
生成过程：
已有："今天天气"
生成下一个词："很好"
已有："今天天气很好"
生成下一个词："适合"
...

这就是CLM的训练方式，生成时完全一致
```

**原因2：BERT不适合生成**
```
BERT是双向的，预测[MASK]时要看右边
但生成时，右边还没生成出来，没法看

所以BERT做生成很别扭，需要迭代多次
```

---

### Q14：什么是Encoder-only/Decoder-only/Encoder-Decoder？

**三种架构：**

| 架构 | 代表模型 | 注意力 | 适合任务 |
|------|----------|--------|----------|
| Encoder-only | BERT | 双向 | 理解（分类、NER） |
| Decoder-only | GPT | 单向 | 生成（对话、续写） |
| Encoder-Decoder | T5, BART | 编码双向，解码单向 | Seq2Seq（翻译、摘要） |

**图示：**
```
Encoder-only (BERT):
输入 → [Encoder] → 输出表示 → 分类头

Decoder-only (GPT):
输入 → [Decoder] → 下一个词 → [Decoder] → 下一个词 ...

Encoder-Decoder (T5):
输入 → [Encoder] → 编码表示
                      ↓
              [Decoder] → 输出序列
```

---

## 三、Embedding 原理

### Q15：词向量是怎么训练出来的？

**核心思想：分布式假设**
> "一个词的含义由它的上下文决定"
> 
> "You shall know a word by the company it keeps."

**例子：**
```
"我喜欢吃___"  → 苹果、香蕉、西瓜
"___很甜"     → 苹果、香蕉、西瓜

苹果、香蕉、西瓜 出现在相似的上下文中
→ 它们的向量应该相近
```

**训练过程（Word2Vec为例）：**
```
语料："我 喜欢 吃 苹果"

训练目标：
给定"喜欢"，预测周围的词"我"、"吃"
或
给定"我"、"吃"，预测中间的词"喜欢"

通过大量语料训练，相似词的向量会靠近
```

---

### Q16：Word2Vec的两种模式（CBOW vs Skip-gram）

**CBOW（Continuous Bag of Words）：**
```
用上下文预测中心词

上下文：["我", "喜欢", "苹果"]
       ↓
    [模型]
       ↓
中心词："吃"
```

**Skip-gram：**
```
用中心词预测上下文

中心词："吃"
       ↓
    [模型]
       ↓
上下文：["我", "喜欢", "苹果"]
```

**对比：**
| 方面 | CBOW | Skip-gram |
|------|------|-----------|
| 训练速度 | 快 | 慢 |
| 低频词效果 | 一般 | 好 |
| 适用场景 | 大语料 | 小语料 |

**为什么Skip-gram对低频词好？**
- CBOW：低频词作为目标，样本少
- Skip-gram：低频词作为输入，可以生成多个训练样本

---

### Q17：句子Embedding怎么得到？（Pooling策略）

**方法1：CLS Pooling（BERT常用）**
```python
# 取[CLS]位置的向量
sentence_embedding = hidden_states[0]  # [CLS]的向量
```

**方法2：Mean Pooling（最常用）**
```python
# 所有token向量求平均
sentence_embedding = hidden_states.mean(dim=0)
```

**方法3：Max Pooling**
```python
# 每个维度取最大值
sentence_embedding = hidden_states.max(dim=0)
```

**方法4：Attention Pooling**
```python
# 学一个注意力权重
weights = softmax(hidden_states @ W)
sentence_embedding = (weights * hidden_states).sum(dim=0)
```

**哪个最好？**
- 一般情况：Mean Pooling 效果稳定
- BERT微调后：CLS Pooling 更好
- M3E、BGE等：用Mean Pooling

---

### Q18：为什么Embedding可以衡量语义相似度？

**原因：训练目标就是让相似的靠近**

**Word2Vec时代：**
```
"苹果"和"香蕉"经常出现在相似上下文
→ 训练时它们的向量被拉近
→ 余弦相似度高
```

**Sentence Embedding时代：**
```
对比学习训练：
- 正样本对（相似句子）：向量拉近
- 负样本对（不相似句子）：向量推远

训练后：
"我喜欢吃苹果" 和 "我爱吃苹果" → 相似度高
"我喜欢吃苹果" 和 "今天天气好" → 相似度低
```

---

### Q19：余弦相似度 vs 欧氏距离？

**余弦相似度：**
$$\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

- 衡量**方向**的相似度
- 范围：[-1, 1]，1最相似
- **不受向量长度影响**

**欧氏距离：**
$$d = \sqrt{\sum(A_i - B_i)^2}$$

- 衡量**位置**的距离
- 范围：[0, +∞]，0最相似
- **受向量长度影响**

**例子：**
```python
A = [1, 2, 3]
B = [2, 4, 6]  # A的2倍
C = [1, 0, 0]

# 余弦相似度
cos(A, B) = 1.0   # 方向相同
cos(A, C) = 0.27  # 方向不同

# 欧氏距离
dist(A, B) = 3.74  # 有距离
dist(A, C) = 3.32  # 也有距离
```

**为什么Embedding常用余弦相似度？**
1. 语义相似度更关心"方向"而非"长度"
2. 归一化后，余弦相似度 = 点积，计算快
3. 向量长度可能受词频等因素影响，余弦可以消除

---

## 四、对比学习

### Q20：什么是对比学习？

**核心思想：拉近相似的，推远不相似的**

**训练过程：**
```
1. 构造正样本对（相似）：(x, x+)
2. 构造负样本对（不相似）：(x, x-)
3. 优化目标：让 sim(x, x+) > sim(x, x-)
```

**图示：**
```
        推远 ←          → 推远
          x-            x-
           ↖           ↗
             \       /
              \     /
        x ←——拉近——→ x+
```

**为什么有效？**
- 不需要标签，只需要知道哪些"相似"
- 学到的表示具有判别性

---

### Q21：InfoNCE Loss是什么？

**公式：**
$$\mathcal{L} = -\log \frac{\exp(\text{sim}(x, x^+)/\tau)}{\sum_{i=0}^{N}\exp(\text{sim}(x, x_i)/\tau)}$$

**直觉理解：**
```
N+1个样本中有1个正样本、N个负样本
任务：从N+1个中识别出正样本
Loss：正样本的概率取负对数
```

**代码实现：**
```python
def info_nce_loss(query, positive, negatives, temperature=0.07):
    # query: [1, dim]
    # positive: [1, dim]  
    # negatives: [N, dim]
    
    # 正样本相似度
    pos_sim = cosine_similarity(query, positive) / temperature  # 标量
    
    # 负样本相似度
    neg_sims = cosine_similarity(query, negatives) / temperature  # [N]
    
    # 拼接
    logits = torch.cat([pos_sim, neg_sims])  # [N+1]
    
    # 正样本是第0个
    labels = torch.zeros(1, dtype=torch.long)
    
    # 交叉熵
    loss = F.cross_entropy(logits.unsqueeze(0), labels)
    return loss
```

---

### Q22：正负样本怎么构造？

**正样本构造方法：**

| 方法 | 例子 | 适用场景 |
|------|------|----------|
| 数据增强 | 原句 vs 同义词替换 | 无监督 |
| Dropout | 同一句子过两次（不同Dropout） | SimCSE |
| 人工标注 | 问题 vs 正确答案 | 有监督 |
| 回译 | 中→英→中 | 无监督 |

**负样本构造方法：**

| 方法 | 例子 | 难度 |
|------|------|------|
| In-batch负采样 | 同一batch的其他样本 | Easy |
| 随机采样 | 随机选其他句子 | Easy |
| Hard Negative | 语义相近但不同的 | Hard |

**In-batch负采样（最常用）：**
```python
# Batch内有4个样本，每个有正样本
# x1-x1+, x2-x2+, x3-x3+, x4-x4+

# 对于x1来说：
# 正样本：x1+
# 负样本：x2, x2+, x3, x3+, x4, x4+（其他所有）
```

---

### Q23：温度系数τ的作用？

**公式中的位置：**
$$\text{sim}(x, x^+) / \tau$$

**τ小（如0.05）：**
```python
sims = [0.9, 0.5, 0.3] / 0.05 = [18, 10, 6]
softmax([18, 10, 6]) ≈ [0.9997, 0.0003, 0.00]
# 分布非常尖锐，几乎one-hot
# 只关注最相似的，梯度集中
```

**τ大（如1.0）：**
```python
sims = [0.9, 0.5, 0.3] / 1.0 = [0.9, 0.5, 0.3]
softmax([0.9, 0.5, 0.3]) ≈ [0.44, 0.30, 0.26]
# 分布平滑
# 所有负样本都有梯度
```

**怎么选？**
- τ太小：只学区分最相似的，忽略其他
- τ太大：区分度不够，学得慢
- 常用值：0.05 ~ 0.1

---

### Q24：SimCSE是什么原理？

**核心创新：用Dropout构造正样本**

**无监督SimCSE：**
```python
# 同一个句子过两次Encoder，Dropout不同
sentence = "我喜欢吃苹果"

# 第一次前向
z1 = encoder(sentence)  # Dropout随机关闭一些神经元

# 第二次前向（同一句子）
z2 = encoder(sentence)  # Dropout关闭的神经元不同

# z1和z2作为正样本对
# Batch内其他句子作为负样本
```

**为什么有效？**
1. Dropout相当于轻微的数据增强
2. 同一句子的两个表示应该相近
3. 简单但效果好

**有监督SimCSE：**
```
用NLI数据集：
- 正样本：蕴含关系的句子对
- 负样本：矛盾关系的句子对
```

---

## 五、大模型基础

### Q25：什么是KV Cache？

**问题：生成时的重复计算**

```
生成过程：
Step 1: "今天" → Q1, K1, V1 → Attention → "天气"
Step 2: "今天 天气" → Q1,Q2, K1,K2, V1,V2 → Attention → "很好"
Step 3: "今天 天气 很好" → Q1,Q2,Q3, K1,K2,K3, V1,V2,V3 → ...

问题：每一步都重新算之前所有位置的K、V，浪费！
```

**解决：缓存K、V**

```python
# 维护一个cache
kv_cache = []

# Step 1
k1, v1 = compute_kv("今天")
kv_cache.append((k1, v1))

# Step 2: 只算新token的K、V
k2, v2 = compute_kv("天气")
kv_cache.append((k2, v2))
# Attention时用cache里的所有K、V

# Step 3: 同理
k3, v3 = compute_kv("很好")
kv_cache.append((k3, v3))
```

**效果：**
- 时间复杂度从O(n²)降到O(n)
- 但显存占用增加（要存所有K、V）

---

### Q26：什么是Flash Attention？

**问题：标准Attention显存占用大**

```python
# 标准计算
Q, K, V = ...  # [seq_len, d]
scores = Q @ K.T  # [seq_len, seq_len] ← 这个矩阵很大！
attn = softmax(scores)
output = attn @ V
```

当seq_len=4096时，scores矩阵占用 4096×4096×4bytes = 64MB（单个头）

**Flash Attention的解决方案：分块计算**

```
不一次性算出完整的scores矩阵
而是分块计算，每次只处理一小块
利用GPU SRAM（快但小）而非HBM（慢但大）
```

**核心技巧：**
1. **Tiling**：把Q、K、V分成小块
2. **重计算**：不存中间结果，反向时重新算
3. **在线Softmax**：分块计算Softmax

**效果：**
- 显存占用大幅降低
- 速度更快（IO优化）
- 支持更长序列

---

### Q27：模型量化是什么？（INT8/INT4）

**问题：模型太大**
```
Qwen-7B：7B参数 × 4bytes(FP32) = 28GB
一张V100(16GB)装不下
```

**解决：用更少的位数表示参数**

| 精度 | 每参数字节 | 7B模型大小 | 精度损失 |
|------|-----------|------------|----------|
| FP32 | 4 | 28GB | 无 |
| FP16/BF16 | 2 | 14GB | 很小 |
| INT8 | 1 | 7GB | 小 |
| INT4 | 0.5 | 3.5GB | 中等 |

**量化过程（INT8为例）：**
```python
# 原始FP16权重
W = [-0.5, 0.3, -0.1, 0.8]  # 范围约[-1, 1]

# 找到scale
scale = max(abs(W)) / 127  # INT8范围是[-128, 127]

# 量化
W_int8 = round(W / scale)  # [-64, 38, -13, 102]

# 推理时反量化
W_approx = W_int8 * scale  # 近似原始值
```

**常用方法：**
- **GPTQ**：逐层量化，考虑激活值
- **AWQ**：保护重要权重
- **bitsandbytes**：LLM.int8()

---

### Q28：LoRA微调原理？

**问题：全参数微调成本高**
```
7B模型微调：
- 需要存储7B参数的梯度
- 需要存储优化器状态（Adam需要2倍参数）
- 显存占用巨大
```

**LoRA的思路：冻结原参数，只训练低秩增量**

```
原始：Y = X @ W        (W是d×d的大矩阵)

LoRA：Y = X @ W + X @ A @ B
      
      W: 冻结不训练
      A: d×r 的小矩阵（r<<d，如r=8）
      B: r×d 的小矩阵
      
      增量 = A @ B，是低秩矩阵
```

**图示：**
```
        ┌─────────────┐
    X → │  W (冻结)    │ → 
        └─────────────┘
              +
        ┌───┐   ┌───┐
    X → │ A │ → │ B │ → 
        └───┘   └───┘
        (d×r)   (r×d)
```

**为什么有效？**
1. 微调的改变量往往是低秩的
2. 只需训练 2×d×r 个参数，远小于 d×d
3. 推理时可以合并：W' = W + A@B，无额外开销

**参数量对比：**
```
原始7B：7,000,000,000 参数
LoRA (r=8)：约 10,000,000 参数（<1%）
```

---

### Q29：什么是Prompt Engineering？

**定义：通过设计输入提示词来引导模型输出**

**常用技巧：**

**1. 角色设定**
```
你是一个专业的汽车工程师...
```

**2. 少样本示例（Few-shot）**
```
问题：什么是智能座舱？
答案：智能座舱是指...

问题：怎么连接蓝牙？
答案：
```

**3. 思维链（Chain of Thought）**
```
请一步步思考：
1. 首先分析问题...
2. 然后考虑...
3. 最后得出结论...
```

**4. 输出格式约束**
```
请以JSON格式输出：
{"answer": "...", "confidence": 0.9}
```

**5. 负面约束**
```
不要编造信息，如果不知道请说"我不确定"
```

---

### Q30：什么是幻觉（Hallucination）？怎么缓解？

**定义：模型生成看起来合理但实际错误的内容**

**例子：**
```
问题：爱因斯坦是哪年获得诺贝尔物理学奖的？
幻觉答案：爱因斯坦于1905年因相对论获得诺贝尔物理学奖
正确答案：1921年，因光电效应
```

**幻觉的原因：**
1. 训练数据中的错误信息
2. 模型倾向于生成流畅的文本，而非正确的
3. 长尾知识记忆不牢

**缓解方法：**

| 方法 | 原理 | 效果 |
|------|------|------|
| RAG | 用检索的文档作为依据 | ⭐⭐⭐⭐ |
| Prompt约束 | "只根据文档回答" | ⭐⭐⭐ |
| Self-consistency | 多次生成取一致的 | ⭐⭐⭐ |
| 低温度采样 | temperature低，更确定 | ⭐⭐ |
| 知识图谱验证 | 用KG校验事实 | ⭐⭐⭐⭐ |

**RAG为什么能缓解幻觉？**
```
不RAG：模型凭记忆回答（可能记错）
用RAG：模型根据检索到的文档回答（有据可依）

Prompt: "根据以下文档回答：{documents}"
```

---

## 六、向量检索

### Q31：FAISS的索引类型有哪些？

| 索引 | 原理 | 速度 | 精度 | 适用规模 |
|------|------|------|------|----------|
| Flat | 暴力搜索 | 慢 | 100% | <10万 |
| IVF | 聚类+倒排 | 快 | 95%+ | 10万~1000万 |
| HNSW | 图索引 | 很快 | 98%+ | 10万~1000万 |
| PQ | 乘积量化 | 很快 | 90%+ | >1000万 |

**代码示例：**
```python
import faiss

# Flat（精确搜索）
index = faiss.IndexFlatIP(dim)  # 内积
index = faiss.IndexFlatL2(dim)  # L2距离

# IVF（聚类加速）
nlist = 100  # 聚类中心数
quantizer = faiss.IndexFlatL2(dim)
index = faiss.IndexIVFFlat(quantizer, dim, nlist)
index.train(vectors)  # 需要训练

# HNSW（图索引）
index = faiss.IndexHNSWFlat(dim, 32)  # 32是M参数
```

---

### Q32：什么是ANN（近似最近邻）？

**精确最近邻（KNN）：**
- 遍历所有向量，计算距离，找最近的K个
- 时间复杂度：O(N)
- N很大时太慢

**近似最近邻（ANN）：**
- 用索引结构加速
- 可能漏掉真正最近的
- 但速度快得多

**Trade-off：**
```
精度 ←──────────────→ 速度

Flat (100%, 慢) ────→ IVF (95%, 快) ────→ PQ (90%, 很快)
```

**衡量指标：Recall@K**
```
Recall@10 = 在近似搜索的Top10中，有多少是真正的Top10
```

---

### Q33：IVF索引的原理？

**思路：先粗筛，再精排**

**1. 训练阶段：对向量聚类**
```
把所有向量分成nlist个簇
每个簇有一个中心点
```

**2. 插入阶段：向量归入最近的簇**
```
新向量 → 找最近的中心 → 归入该簇
```

**3. 搜索阶段：只搜索最相关的几个簇**
```
Query → 找最近的nprobe个簇中心 → 只在这些簇内搜索
```

**图示：**
```
         ○ ← 查询向量
        /|\
       / | \
      ●  ●  ●  ← 只搜索最近的3个簇（nprobe=3）
     /|  |  |\
    ○○ ○○ ○○○○ ← 簇内的向量
```

**参数：**
- `nlist`：聚类数，越大越准但越慢
- `nprobe`：搜索时查几个簇，越大越准但越慢

---

### Q34：HNSW索引的原理？

**HNSW = Hierarchical Navigable Small World**

**思路：构建多层图，快速导航**

**图结构：**
```
Layer 2:    ○───────────────○    （稀疏，跳跃大）
            │               │
Layer 1:    ○───○───────○───○    （中等）
            │   │       │   │
Layer 0:    ○─○─○─○─○─○─○─○─○    （稠密，所有向量）
```

**搜索过程：**
```
1. 从最高层的入口点开始
2. 在当前层贪心搜索，找到最近的
3. 下降到下一层，继续搜索
4. 直到最底层，返回结果
```

**类比：**
- 像跳表（Skip List）
- 高层是"高速公路"，快速接近目标
- 低层是"普通道路"，精确定位

**优点：**
- 不需要训练
- 查询速度快
- 精度高

**缺点：**
- 内存占用大（要存图结构）
- 构建时间长

---

## 七、损失函数

### Q35：交叉熵损失是什么？

**用于分类任务**

**公式：**
$$\mathcal{L} = -\sum_{i} y_i \log(p_i)$$

- $y_i$：真实标签（one-hot）
- $p_i$：预测概率

**例子：**
```python
# 3分类问题
真实标签：y = [0, 1, 0]  # 第2类
预测概率：p = [0.1, 0.7, 0.2]

# 交叉熵
L = -(0*log(0.1) + 1*log(0.7) + 0*log(0.2))
  = -log(0.7)
  = 0.36
```

**直觉：**
- 让正确类别的预测概率尽量大
- 正确类别概率越高，Loss越低

**代码：**
```python
import torch.nn.functional as F

logits = model(x)  # [batch, num_classes]
labels = ...       # [batch]

loss = F.cross_entropy(logits, labels)
```

---

### Q36：Triplet Loss是什么？

**用于学习Embedding**

**公式：**
$$\mathcal{L} = \max(0, d(a, p) - d(a, n) + margin)$$

- a：Anchor（锚点）
- p：Positive（正样本）
- n：Negative（负样本）
- d：距离函数
- margin：边界值

**目标：**
```
d(anchor, positive) + margin < d(anchor, negative)

即：正样本比负样本更近，而且至少近margin距离
```

**图示：**
```
      n（负样本）
      ↑
      │ > margin + d(a,p)
      │
      a（锚点）───── p（正样本）
              d(a,p)
```

**代码：**
```python
import torch.nn.functional as F

loss = F.triplet_margin_loss(
    anchor,    # [batch, dim]
    positive,  # [batch, dim]
    negative,  # [batch, dim]
    margin=1.0
)
```

---

### Q37：Contrastive Loss vs Triplet Loss？

**Contrastive Loss（对比损失）：**
```
输入：一对样本 (x1, x2) + 标签（相似/不相似）

Loss = {
    d(x1, x2)²                     如果相似
    max(0, margin - d(x1, x2))²    如果不相似
}
```

**Triplet Loss（三元组损失）：**
```
输入：三个样本 (anchor, positive, negative)

Loss = max(0, d(a,p) - d(a,n) + margin)
```

**对比：**

| 方面 | Contrastive | Triplet |
|------|-------------|---------|
| 输入 | 样本对 | 三元组 |
| 负样本 | 需要明确标记 | 隐式（相对关系） |
| 常用于 | 验证型任务 | 检索型任务 |

**InfoNCE vs 两者：**
- InfoNCE = 多负样本版本的对比学习
- 一个正样本 + N个负样本
- 效果通常更好

---

### Q38：为什么用Softmax做分类？

**Softmax的作用：把logits变成概率分布**

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$

**性质：**
1. **输出范围[0,1]**：可以解释为概率
2. **和为1**：满足概率分布要求
3. **可微**：可以反向传播

**例子：**
```python
logits = [2.0, 1.0, 0.1]

# Softmax
exp_logits = [e^2, e^1, e^0.1] = [7.39, 2.72, 1.11]
sum = 11.22
probs = [0.66, 0.24, 0.10]

# 最大的logits对应最大的概率
```

**为什么不直接归一化？**
```python
# 直接归一化
logits = [2.0, 1.0, 0.1]
probs = [2/3.1, 1/3.1, 0.1/3.1] = [0.65, 0.32, 0.03]

# 问题1：负数怎么办？
logits = [2.0, -1.0, 0.1]  # 负数不能做概率

# Softmax用exp保证正数
```

**温度的作用：**
```python
softmax(logits / T)

T小 → 分布尖锐（接近one-hot）
T大 → 分布平滑（接近均匀）
```

---

## 总结

| 类别 | 核心知识点 | 与RAG项目的关联 |
|------|------------|----------------|
| Transformer | Self-Attention, Multi-Head, FFN | BERT/GPT的基础 |
| BERT vs GPT | 双向/单向, MLM/CLM | M3E基于BERT, Qwen基于GPT |
| Embedding | Word2Vec, Pooling, 相似度 | 召回模块的核心 |
| 对比学习 | InfoNCE, 正负样本, SimCSE | Embedding微调 |
| 大模型 | KV Cache, 量化, LoRA | vLLM部署优化 |
| 向量检索 | FAISS, IVF, HNSW | 召回模块 |
| 损失函数 | 交叉熵, Triplet, InfoNCE | 训练的基础 |

---

## 面试Tips

1. **公式要会推**：Attention公式、InfoNCE公式要能手写
2. **直觉要清晰**：每个概念用一句大白话解释
3. **代码要熟悉**：至少能写出Attention和InfoNCE的伪代码
4. **与项目关联**：被问到八股时，主动关联到你的项目
