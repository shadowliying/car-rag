# 项目实战1：xx智能座舱汽车知识问答系统

## 目录

- [项目实战1：xx智能座舱汽车知识问答系统](#项目实战1xx智能座舱汽车知识问答系统)
  - [目录](#目录)
      - [项目背景：](#项目背景)
      - [工作内容：](#工作内容)
      - [项目亮点：](#项目亮点)
    - [面试官可能问的问题](#面试官可能问的问题)
      - [1.介绍一下这个项目](#1介绍一下这个项目)
      - [2.做这个项目有没有遇到什么问题，是怎样解决的？](#2做这个项目有没有遇到什么问题是怎样解决的)
      - [3.那你讲一下什么是continous batching](#3那你讲一下什么是continous-batching)
      - [4.检索模型是如何选型的，有什么考虑？](#4检索模型是如何选型的有什么考虑)
      - [5.为什么分片时要用不同的滑动窗口长度对数据做处理？](#5为什么分片时要用不同的滑动窗口长度对数据做处理)
      - [6.为什么要用Faiss做向量检索，还用过其他的向量数据库吗？](#6为什么要用faiss做向量检索还用过其他的向量数据库吗)
      - [7.能否简要介绍一下Faiss检索的核心原理](#7能否简要介绍一下faiss检索的核心原理)
      - [8.测试集是如何构造出来的，讲一下过程？](#8测试集是如何构造出来的讲一下过程)
      - [9.系统问答的效果是如何评估的？](#9系统问答的效果是如何评估的)
      - [10.四张GPU卡做分布式部署，这块具体怎么做的？](#10四张gpu卡做分布式部署这块具体怎么做的)
      - [11.pdf解析用的什么工具，了解其他解析工具吗？](#11pdf解析用的什么工具了解其他解析工具吗)
      - [12.这个项目的亮点是什么？](#12这个项目的亮点是什么)
      - [13.BM25召回的底层原理是什么，你有了解吗？](#13bm25召回的底层原理是什么你有了解吗)
      - [14.有尝试对回复生成模型LLM做过微调吗？效果怎么样？](#14有尝试对回复生成模型llm做过微调吗效果怎么样)
      - [15.你用过vLLM，那你了不了解它的底层原理，讲一下Paged Attention?](#15你用过vllm那你了不了解它的底层原理讲一下paged-attention)
      - [16.Rerank重排部分是怎么考虑的，实测有提升吗？](#16rerank重排部分是怎么考虑的实测有提升吗)
      - [17.为什么部署的时候没有考虑对模型做量化？](#17为什么部署的时候没有考虑对模型做量化)
      - [18.模型服务有没有做过压测，是怎么做的？](#18模型服务有没有做过压测是怎么做的)
      - [19.用户手册的图片有没有考虑进来，如果要把答案相关的图片展示出来，你准备怎么做？](#19用户手册的图片有没有考虑进来如果要把答案相关的图片展示出来你准备怎么做)
      - [20.如果系统回答出现了badcase，如何快速定位并修复？](#20如果系统回答出现了badcase如何快速定位并修复)
      - [21. 如果想让LLM生成的回答，每句话都标明它的来源，应该怎么做？例如AI中，大模型回答的答案都有\[1\] \[2\] \[3\]这种类似论文的参考链接。这是如何实现的？](#21-如果想让llm生成的回答每句话都标明它的来源应该怎么做例如ai中大模型回答的答案都有1-2-3这种类似论文的参考链接这是如何实现的)
      - [22.PDF解析滑窗法有哪些问题？如何改进？](#22pdf解析滑窗法有哪些问题如何改进)
      - [23.这个项目QPS并发大概多少？](#23这个项目qps并发大概多少)
      - [24.Rerank部分有做微调吗，是怎么考虑的？](#24rerank部分有做微调吗是怎么考虑的)
      - [25.表格解析过程中遇到的问题和解决方法？面试官追问我这些库解析出来的结果是怎么处理的，还要人工筛选一遍吗？](#25表格解析过程中遇到的问题和解决方法面试官追问我这些库解析出来的结果是怎么处理的还要人工筛选一遍吗)
      - [26.字节面试官问了我两个场景题：第一个是如何去解决数据分布的长尾问题或者多样性不够的问题，](#26字节面试官问了我两个场景题第一个是如何去解决数据分布的长尾问题或者多样性不够的问题)
      - [27.召回模型都用的是什么呢？   具体原理。    ](#27召回模型都用的是什么呢---具体原理----)
      - [
28.精排微调前后的效果对比，精排模型具体是怎么做的？](#28精排微调前后的效果对比精排模型具体是怎么做的)
      - [
29.有没有考虑过有的query长尾分布的情况下效果不会很好，或者十句话压缩到一个embedding里面的话，其实有可能差别不是很大呢？有看过实际的embedding吗？](#29有没有考虑过有的query长尾分布的情况下效果不会很好或者十句话压缩到一个embedding里面的话其实有可能差别不是很大呢有看过实际的embedding吗)
      - [
30.召回的阈值是怎么设置的呢?](#30召回的阈值是怎么设置的呢)
      - [
31.意图分类模块是不是分的类别有点宽泛了？这会不会导致数据前期就分错呢？你这个有测试过吗？](#31意图分类模块是不是分的类别有点宽泛了这会不会导致数据前期就分错呢你这个有测试过吗)
      - [
32.你做的query改写泛化是不是有点宽泛？](#32你做的query改写泛化是不是有点宽泛)

#### 项目背景：

构建了一个基于xx汽车用户手册的智能座舱知识问答系统，通过结合LLM，Langchain和提示工程，优化知识库结构和检索生成流程，提升答案生成的准确性，快速精准的回答用户汽车相关问题，实现高效的领域车知识问答体验。

#### 工作内容：

- 先用pdfminer识别出文档块，然后利用文档分块和滑动窗口解析法，用不同窗长对xx汽车用户手册进行解析，保证了文本内容的完整行和跨页连续性；
- 采用多路召回策略，使用Dense语义召回M3E，Sparse语义召回BGE-M3，以及字面召回TF-IDF和BM25多路混合召回策略，Faiss实现向量检索，同时用BGE-Reranker对候选答案进行精排；
- 采用了ChatGLM3-6B和Qwen2-7B-Chat，GPT-4o等LLMs作为大模型基座，并实现了插件可配置化，提升了系统的灵活性和可扩展性；
- 人工构造2000条测试集，依据相似度和关键词两项综合加权评分，问答系统准确率达到89.6%，相比原生GPT4外挂知识库提升4.1%。

#### 项目亮点：

- 优化提示工程，在输入侧做query的纠错、改写、规范化和扩展，召回过程中的优化杂乱不通顺的候选知识规整，以及最终输出做后处理，降低输出的不合理case，问答准确率提升3.7%。
- 对项目工程代码做docker容器化部署，并利用vLLM框架对Qwen2大模型进行推理加速，实现4卡v100分布式部署，极大提升了推理效率，首字延迟降低45%，吞吐率达到12K token/s。

### 面试官可能问的问题

#### 1.介绍一下这个项目

答：好的，这个项目是构建了一个基于领克汽车用户手册的智能座舱知识问答系统，通过结合LLM，Langchain和提示工程和vLLM框架，精准回答用户汽车相关问题。项目完整实现了RAG的分片，检索，多路召回，重排序，答案生成，模型部署完整的流程，最终实现了高效的领域车知识问答体验。

#### 2.做这个项目有没有遇到什么问题，是怎样解决的？

答：是有的。做这个项目踩了一些坑，也积累了处理实际问题的经验。印象比较深的有两个。第一个是项目完成初版之后，通过测试发现，系统对某些口语化的表述，例如“加几升油”。通过优化提示工程对query做改写和规范化，优化后输出完整的话，“油箱加满油是多少升”。另外，对某些专业名词缩写整理了一个<缩写，中文全称>的同义词映射表，对输入做改写。这些优化显著提升了口语化，省略半截句的召回效果。另外，第二个是发现当多用户并发测试时，吐字比较慢，性能出现瓶颈。优化的手段就是用伯克利的vLLM框架对LLM做加速，因为框架实现了paged attention, continuous batching等优化技术，所以推理效率明显有了提升。另外，利用多卡资源以及docker容器化部署，实现节点横向扩展，进一步提高了压测下的可接受的请求用户数，同时保证用户的QoS。

#### 3.那你讲一下什么是continous batching

传统的Batching是模型粒度的，将多个请求合并在一起，然后调用模型的推理接口，完成后在解batch，将结果返回给各个请求。这种方式对于大语言模型推理并不适用，因为大模型的推理有2个特点：1.生成长度不可知 2.一个请求的时间可能很长

Continous batching 采用了Iteration式的调度方式，其中batch大小根据每次迭代确定。这里的 Iteration 指一次 decode 计算。也就是说在每次 decode 的迭代过程中，做 batch 的调度调整。一旦batch中的一个序列完成生成，就可以在其位置插入一个新的序列，从而实现比静态批处理更高的GPU利用率。

#### 4.检索模型是如何选型的，有什么考虑？

答：主要有两点考虑，一是检索效率，二是检索效果。首先稠密向量的存储和检索需要更多的计算资源，尤其是当数据集非常大时。稠密检索在计算相似度时更准确，因为它们能够更好地处理同义词和上下文依赖。稀疏检索在计算上通常更高效，因为稀疏向量可以利用特殊的数据结构（例如倒排索引）进行快速检索，而且存储成本较低。这种表示方法只考虑了词汇的出现频率，忽略了词汇的语义关系和上下文信息。最后项目综合了稠密检索和稀疏检索的优点，采用混合多路检索+重排的方式，一方面可以明显提高检索的准确率，确保有用信息不遗漏，另一方面稠密向量采用Faiss高效的向量检索工具，也克服了检索效率的问题。

#### 5.为什么分片时要用不同的滑动窗口长度对数据做处理？

答：这个原因是因为我发现在文本切片的时候，容易把连续信息切碎，导致信息不完整。例如操作类的连续多步骤，数字指标类，还有长专有名词类，这个会对模型做答案生成产生较大影响。因此我采用了滑动窗口+变长度的思路。首先窗口之前有overlap，保证信息的一定连贯性。另外，设置不同长度的切片，因为embedding模型对不同长度的输入表征能力不一样，通过设置不同的输入长度再embedding向量化之后，也能产生不同粒度的更丰富的语义信息。

#### 6.为什么要用Faiss做向量检索，还用过其他的向量数据库吗？

答：当时有考虑过向量数据的选型。因素主要考虑这几点：数据量大小，检索效率和落地成熟度。这个项目的数据主要来源于PDF用户用车手册，所以数据量属于偏小的。另外，faiss是facebook开发的一个高效的数据检索库，在NLP，图像，视频检索领域已有大量应用，成熟度比较高，也支持GPU索引，检索效率和准确率属于中上水平。综合数据，效率和成熟度多个因素考虑，最终选择了Faiss检索库。我还用过其他的向量数据库，例如Chroma和Milvus。两个数据库都比较新，Chroma优化了查询处理过程，实时性能优化做得比较好，Milvus生态做得好，可以无缝集成深度学习框架和模型，也比较适合处理大规模的数据集，具备高可用性架构，可以水平扩展以适应不断增长的数据量。

#### 7.能否简要介绍一下Faiss检索的核心原理

答：Faiss的核心原理是**基于索引和查询操作**。 在索引阶段，Faiss将所有输入的向量转换为指定的维度，并通过聚类算法将它们分组。 每个聚类中心都被视为一个原型向量，并用于构建索引。 这一步步骤也叫向量量化。在查询阶段，Faiss通过计算输入向量与所有原型向量的距离，快速找到最相似的聚类中心，从而完成相似向量检索。这个过程也是Faiss利用倒排文件系统来实现的。总结一下就是FAISS的在于利用量化技术和倒排索引来减少搜索空间，提高检索速度，同时尽可能保持检索的准确性。

#### 8.测试集是如何构造出来的，讲一下过程？

答：我从手册里人工构造了600多条问题，覆盖了几个维度：数字类，例如问后备箱容积有多大，总结类，例如这个车的亮点有哪些。话术有标准话术，也有口语化的，半截句省略的话术。然后再给这些问题生成答案，采用的思路是利用商用通义千问的RAG接口来生成答案，调研了几家，通义的效果是相对较好的。生成答案后再人工去检查答案的质量，对不准确和不通顺的回答进行调整和改写。最后再针对这些原始问题用提示+GPT4做query泛化，以此来验证模型的召回的稳定性。最后泛化完，经过人工筛选，保留2000条作为最后的测试集。

#### 9.系统问答的效果是如何评估的？

答：因为是生成类问题，没有统一标准的答案。因此在做评测的时候，我主要考虑了2个方面。一是答案的关键信息，例如关键的实体，发动机，玻璃水，引擎盖等。跟问题相关的数字不能偏离太多，例如轮胎尺寸，后备箱大小。车身尺寸，车机屏幕尺寸等。这些关键信息会作为关键词来匹配和判断。第二，从语义上不能偏离给定答案太远。这里就采用了文本相似度来判断。因此整体上评测采用了两部分加权组合：Score=0.5\*实体词jaccard相似度+0.5\*文本相似度得分。其中文本相似度采用了text2vec-large-chinese作为向量模型。

#### 10.四张GPU卡做分布式部署，这块具体怎么做的？

答：首先，我当时做的时候考虑过两种技术方案，因为目的是提升问答服务的的并发能力，一种思路是利用vLLM的分布式部署，把一个模型部署到2张卡上，两张卡之间推理可以采用张量并行，速度会快一些，共部署两个节点。第二种思路是直接部署4个节点，因为是7b的模型所以单卡部署也是够的。最终经过做完压力测试，对比了下性能，发现总吞吐率还是直接部署4个节点效率更高。要把请求均匀的打在4个节点上，就涉及负载均衡的问题，这里我没有采用K8S的负载均衡方案，即round-robin，随机分发请求，而是考虑了GPU利用率的情况。原因是根据query不同，有些请求回答很长，有些很短，所以持续占用GPU的时间是不同的。因此这里分发做了一个调度策略，我写了一个代理服务，由这个代理服务来做负载均衡，这个代理会监控每个GPU的利用率，优先把新任务分发给利用率较低的GPU，这样就保证了每块GPU卡的任务量是均衡的，避免出现木桶短板的情况。

#### 11.pdf解析用的什么工具，了解其他解析工具吗？

答：我这边是采用的是paddleOCR按页来处理 pdf 的，获得页面的所有文档块和表格，我这边也调研过python的库pdfplumber，它是基于PDFMiner和PyPDF2等底层库，但提供了更高层次的抽象和更友好的API，开发起来更容易上手，但是在处理分栏的pdf还有图文混排，效果不好。我还了解pdfminer3k，是 pdfminer 的 python3 版本，主要用于读取 pdf 中的文本。但是对于表格的处理非常的不友好，能提取出文字，但是没有格式。还有tabula，tabula 是专门用来提取PDF表格数据的，但是这工具是用 java 写的，依赖 java8。tabula-py 就是对它做了一层 python 的封装，所以也依赖 java8。还有camelot，用于PDF表格解析的工具，能够处理复杂表格布局。实际写代码时，可以几种工具结合着一起用。

#### 12.这个项目的亮点是什么？

答：项目的亮点主要有两个方面：一是在PE上的优化，我分别在输入，召回还有输出侧都用PE做了数据的优化。首先在输入侧做了query的纠错、改写、规范化和扩展，召回过程中的优化杂乱不通顺的候选知识规整，以及最终输出做后处理，降低输出的不合理case，指标有较明显的提升。另一方面，我还对项目工程代码做docker容器化部署，并利用vLLM框架对Qwen2大模型进行推理加速，实现4卡v100分布式部署，极大提升了推理效率，首字延迟降低45%，吞吐率达到12K token/s。/

#### 13.BM25召回的底层原理是什么，你有了解吗？

答：BM25召回最早是用于全文检索引擎的。它本质是TF-IDF一种改进，用于评估文档与给定查询的相关性，从而对文档进行排序。BM25算法考虑了查询词在文档中的频率、在整个文档集合中的频率，以及文档的长度等因素。对于一个特定的查询项，它在相关文档中出现的频率高于在非相关文档中的频率。 算法通过结合词项频率（TF）和文档频率（DF）来计算文档的得分。高IDF值表明词项稀有，因此更可能与特定查询相关。同时，词项在文档中的频率和查询中的频率也被考虑进来，以确保频繁出现在查询中的词项能够得到足够的重视。

#### 14.有尝试对回复生成模型LLM做过微调吗？效果怎么样？

答：你好，我这边尝试过对Qwen-7B的答案生成模型做过微调。我构造了4000多条\[Q, A]训练语料。具体构造方式采用的是先写一些种子问题，然后利用通义千问RAG接口生成答案，然后继续用GPT4对问题和答案进行泛化，生成更多的样本。另外，我还利用GPT4，输入上下文长文本，让它来生成问题和答案，经过人工筛选，构造更多的问题和答案。构造完训练集后，SFT微调Qwen-7B模型，结果发现，微调后的模型在答案生成可控，以及格式方面，比不微调要好，格式和字数都控制得住，但是总结能力变差了。特别是需要对长上下文总结的来得到的答案，比不微调还要差一些，过数据的epoch只能过少一点1～2个epoch，太多就overfit了，效果会下降得很多。我思考了一下，主要原因可能是跟数据的质量和数量有关系，过度微调会使得让原始大模型失去一些通用泛化能力，因此最后没有采用微调的方案。后续有时间可以考虑在数据的多样性以及数据量上面下点功夫，可能会拿到更好的效果。

#### 15.你用过vLLM，那你了不了解它的底层原理，讲一下Paged Attention?

答：PagedAttention 将 Sequence 中 KV Cache 划分为 KV 块，就像内存分页那样，每个块的容量是固定的。对于 PagedAttention 中的 KV 块来说，每个块可以存固定数量 tokens 的K和V Tensor，从而将注意力机制的计算转换为块级的计算。在的大语言模型主要是预测下一个 token，PagedAttention 是将 KV Cache 以块的方式存储起来，每个块就是一个 Block，每个 BlockSize 一般为 16。vLLM采用一个 centralized scheduler 来统筹、分配不通 GPU 工作站的内存管理。中间核心的 KV Cache Manager 在每个 paged fashion 中高效地管理 KV Cache。KV blocks 实际存储的物理内存可能并不是连续的，但是 PagedAttention 会将它们的逻辑 KV blocks 组织为连续的，这样便于上层的使用，这中间会涉及到 KV blocks 的内存映射 ，这种内存映射有点像链表的数据结构。这样做的好处有挺多，一方面可以高效地管理动态增长的 KV Cache 内存，因为增加的内存是不定的，如果是连续开辟内存难免会产生多余的浪费；另外还可以很好地将琐碎的内存片段利用起来，这也是为什么 BlockSize 设定为 16 的一部分原因。

#### 16.Rerank重排部分是怎么考虑的，实测有提升吗？

答：我这边做过不加Rerank和加Rerank模块效果的对比，实测发现有Rerank回答的质量会更好。将 Rerank 整合到 RAG 应用中为什么会work，我想的原因是因为 Rerank 能够在单路或多路的召回结果中挑选出和问题最接近的文档。此外，扩大检索结果的丰富度（例如多路召回）配合精细化筛选最相关结果（Reranker）还能进一步提升最终结果质量。使用 Reranker 可以排除掉第一层召回中和问题关系不大的内容，将输入给大模型的上下文范围进一步缩小到最相关的一小部分文档中。通过缩短上下文， LLM 能够更“关注”上下文中的所有内容，避免忽略重点内容，还能节省推理成本。向量召回中使用的是bi-encoder结构，而bge-reranker-large 使用的是 cross-encoder结构，cross-encoder结构一定程度上要优于bi-encoder。相比于只进行向量检索的基础架构的 RAG，增加 Reranker 也会带来一些挑战，增加一些使用成本，不过这个cost相比答案生成来说，占比很少。

#### 17.为什么部署的时候没有考虑对模型做量化？

答：我这边做过对qwen-7b做不同bit量化下的对比，包括int4，int8量化。从结果上来看，int4量化后的模型，答案总结的效果会差一点，int8和不做量化的模型差别基本不大。速度上，这边再v100上实测，int8速度是相对最快的。现在量化是部署环节比较常用的操作，特别是在端侧的部署。在这个项目中，因为访问量不大，并且显存是足够的，所以没有对base模型进一步做量化。如果用户访问增加，也会考虑把模型做底比特量化，这样可以部署更多的服务实例，从而提高并发，同时还可以节省GPU显存。

#### 18.模型服务有没有做过压测，是怎么做的？

答： 问答服务是有做个子服务和端到端的压测的，子服务压测工具用的locust，端到端服务压测手写的多线程client。首先对embedding模型，rerank模型做了压力测试，评估在不同并发下，这两个模型的响应时长，RPS等指标的变化情况。对于端到端的测试，我采用的是用多线程模拟多用户的测试思路，最后统计首字平均延迟，以及吞吐率token/s的性能指标。除此之外，我还对回复生成模型做了batch的请求测试，测试在不同batch size下模型的吞吐率的变化情况。综合结论是embedding模型是几乎不耗时的，rerank模型由于是cross-encoder的结构，耗时要略高于embedding模型。LLM模型的在vllm batch推理模式下，在batch=1和batch=2下的时间差别较大，但batch=2,4,8,16，差别并不是很大。而在多用户并发的情况下，vLLM由于有continous batching技术，所以吞吐率可以做到很高，多用户访问延迟也增加不多。

#### 19.用户手册的图片有没有考虑进来，如果要把答案相关的图片展示出来，你准备怎么做？

答：是的，目前只考虑了文本和表格的问答，图片还没有考虑进来。如果要考虑把插图也放到答案里，我大致有两个思路：一是采用多模态的大模型，比如MiniCPM-Llama3-V-2\_5，GPT-4V等开源或者闭源的多模态大模型，这也是比较直接的方案，可以做文档，图片的问答。第二种思路是在这个项目上继续改进，还是用LLM作为基座，但是在回答的时候，把插图插入答案中并显示。具体做法是在parse文档的时候，要把图片也解析出来，同时对每个图片进行唯一编号，然后建一个图片文件服务，并暴露接口，根据图片id获取图片的内容。然后把图片id插入在对应文档中的位置，例如：调节大灯的方式见图：【img-0X123】,可以看出，只需要选转右侧的...，然后让LLM在总结答案的时候要保留图片ID，这可以通过提示工程做到，最后在显示的时候，再根据ID获取真实的图片内容，图文有序混合排布展示出来就可以了。

#### 20.如果系统回答出现了badcase，如何快速定位并修复？

答：首先需要定位badcase出现的原因，主要有这几个方面：1.是否是知识库以内的问题 ？2.如果不是知识库的问题，检索模型是否正确召回？ 3.如果数据也召回了，答案生成模型是否正确总结并输出？

首先，如果不是知识库的问题，而出现了误出，那可以再问答系统前面做一层准入模型，也就是二分类，可以是规则+模型，判断输入的问题是不是一个汽车知识相关的问题。

其次，如果是车知识问题，但检索模型没有正确召回，可以考虑用领域数据调优embedding模型，如果为了快速修复，可以根据query的特征考虑增加其他召回源，可以是规则，也可以是模型。

最后，如果正确召回，但是总结模型没有正确输出。那说明模型的能力不足，在不换模型的情况下，可以考虑优化提示，例如对结果进行二次回答，也可以考虑替换更大一点的LLM模型，总结能力更强的。

#### 21. 如果想让LLM生成的回答，每句话都标明它的来源，应该怎么做？例如AI中，大模型回答的答案都有\[1] \[2] \[3]这种类似论文的参考链接。这是如何实现的？

答：好的。这个问题我想到一种做法。在数据分片入库的时候带上索引id，每个分片是全局唯一的。这个索引号可以回溯该分片的来源信息。例如是哪个文档，多少页，多少行。如果是网页搜索得到的内容，来源可以是对应网址。有了这个id，在LLM生成的时候，可以通过提示约束，让大模型在生成完答案后，同时附上对应的来源id。例如：你是一个xxx系统，你具备xx知识，请回答xx问题，答案需要精准，通顺，字数在多少字以内。并严格按照以下格式输出：{答案}：【{引用编号1}，{引用编号2}......】。如果LLM锁不住格式，则可以采用少量正确引用标注的带格式的数据对模型进行指令微调，最后输出想要的答案格式。

#### 22.PDF解析滑窗法有哪些问题？如何改进？

答：滑动窗口解析法针对不复杂的文档效果很好，例如本项目中的车辆用户手册。但针对较复杂的文档，还是存在一些问题。首先是不能很好的还原PDF文件的文档结构，例如层级的信息，像多级标题、目录等。第二个问题是解决页面布局和错综复杂的格式比较困难，例如纵向/横向合并的复杂表格，双栏图文混排。还有像扫描件的pdf，文字不清晰，只能用OCR技术提取，很多pdf解析库都会失效。

要用一种工具解决pdf解析里面所有问题是比较困难的，但可以针对具体问题来做优化。例如针对双栏图文混排可以采用先用OCR识别文档块，文字，表格，和图片分开提取，并记录相应元素block的位置。这里需要做到几点：对于文字，OCR要确保能够准确地提取PDF文件中的文字内容，并按照正确的顺序进行排列和输出，避免文字乱码，比如可以用百度的PaddleOCR来做。对于表格提取，需要保证表格进行完整性，包括表格内的内容和格式，有专门的表格提取的工具，例如**camelot**，很多OCR工具也支持表格提取。对于图片，提取PDF文件中的图片，并保留其原始质量和格式。把这些子元素都提取正确了，再考虑排版的问题，也就是识别PDF文件的布局，包括页面的排列方式、文本和图片的位置等信息。可以用**pdfminer**来实现。如果有必要，还可以把解析后的结果转换成markdown格式，也就是格式化，这样后续处理起来更简单。

#### 23.这个项目QPS并发大概多少？

答：这个项目利用vLLM框架对Qwen2大模型进行推理加速，实现4卡v100分布式部署，推理效率得到了很大的提升。上线前，我们做过压力测试，最大测试到了512个并发，vllm内部采用了continuous batching技术，所以相当于batch size是512，GPU利用率可以完全打满。因为整个系统是流式的输出，所以性能指标采用的是两种业界常用的指标首字延迟，和平均吞吐率。在最大并发下，首字延迟在2s左右，吞吐率达到12k token/s。

#### 24.Rerank部分有做微调吗，是怎么考虑的？

答：好的，面试官。在这个项目上，rerank这部分微调我这边尝试过，开始并没有取得非常显著的效果，召回提高了0.5%左右。针对这个结果，我这边也分析过原因，其实出在数据上，rerank这部分需要构造pair的偏序数据集。我这边尝试过两种微调数据方案:
一种是pointwise的：query和doc的相关性为二分类（1代表相关、0代表不相关）
另一种是listwise的：query和doc的相关性为多分类。（3，2，1，0，相关性依次降低）

分析了一下原因，问题主要在数据上，首先我尝试了用开源数据，效果基本没提升，原因是开源数据跟我们的领域任务数据分布相差太远了，而且这些开源数据估计BGE-Reranker模型训练的时候也见过了。然后我尝试了我自己手动构造了一些数据，手写偏序对的方式，然后让GPT4写了一部分数据。这部分数据的质量不太高，对于二分类，自己构造出来的数据正例和负例差别太大了，也就是数据太简单了，模型很快就fit了。对于listwise的，构造的数据相关性太高了，没有区分度，造成模型也不太好区分，loss降不下去。我总结了两个原因，一是数据质量不好，要么太简单，要么没有区分度。

所以，后来换了一种思路，直接用真实数据来做，然后对问题做了分层采样，保证各大类别问题都覆盖到。采用把模型召回的候选集来人工打标，有些差别很大的候选，人工增加了一些难负样本，这样花了几天时间，打标了600条左右，最后用这一批数据再去微调，效果明显变得更好了，召回准确率提高了3%左右。

#### 25.表格解析过程中遇到的问题和解决方法？面试官追问我这些库解析出来的结果是怎么处理的，还要人工筛选一遍吗？

答：表格解析遇到过一些问题，例如，如何设计合理的索引结构，以高效组织和存储表格中的关键语义信息。开始用解析出来的原始文本存储，发现效果不好，信息容易产生混淆和缺失。后来采用的是markdown格式来存储，发现比文本效果好很多。不需要人工筛选一遍，解析出来后，把表格转换成markdown，就可以继续走后面流程。

#### 26.字节面试官问了我两个场景题：第一个是如何去解决数据分布的长尾问题或者多样性不够的问题，

答：数据分布长尾问题解决思路：过采样尾部：对尾部数据复制或生成合成样本。欠采样头部：随机删除部分头部数据，平衡分布。加权损失：为尾部类别分配更高损失权重。

多样性不够：一般采用数据增强，采用GPT4来进行语料泛化，基本泛化的效果通过挑选和简单清洗，就可以用。

#### 27.召回模型都用的是什么呢？   具体原理。   &#x20;

答：召回模型用的是bm25召回和bge embedding召回。bm25是稀疏召回的一种，它考虑了单词的IDF权重、文档内部词频的标准化处理以及单词在查询中的重要性。通过计算每个单词的得分并求和，得出文档与查询的整体相关性分数，主要体现的是关键词匹配能力，embedding召回是深度语义召回，本质是向量计算相似度，实际工程是利用近似近邻（ANN）算法来计算，这个在长尾query时，召回具有一定的泛化性。

#### &#xA;28.精排微调前后的效果对比，精排模型具体是怎么做的？

答：精排微调后指标提升3%左右。具体是采用的二分类的微调方案：query和doc的相关性为二分类（1代表相关、0代表不相关），构造领域训练语料，直接用真实数据来做，然后对问题做了分层采样，保证各大类别问题都覆盖到。采用把模型召回的候选集来人工打标，有些差别很大的候选，人工增加了一些难负样本，打标了600条左右，最后用这一批数据再去微调，召回准确率提高了3%左右。

#### &#xA;29.有没有考虑过有的query长尾分布的情况下效果不会很好，或者十句话压缩到一个embedding里面的话，其实有可能差别不是很大呢？有看过实际的embedding吗？

答：对于长尾中的稀有查询，找到相关且准确的信息可能比常见查询更加困难，因为相关信息可能分散在数据源的不同部分，或者数量较少。即使检索到相关信息，也可能存在信息不全面或不足以支持生成准确回答的情况。这个我在embedding中也发现了这个现象，这一类的embedding相似度计算不太准确，embedding出现都差不多的情况。这个问题我也调研过相关解决思路，一种思路是为问题生成假设性答案（HyDE），也就是根据query，利用LLM去生成假设性回答。然后将给出的假设性回答，去做文本的embedding，然后和原始的query一起用于向量检索召回。假设回复可能包含虚假信息，但蕴含着LLM认为相关的信息和文档模式，有助于在知识库中寻找类似的文档。适合长尾的问题，后面也准备尝试一下。

#### &#xA;30.召回的阈值是怎么设置的呢?

答.两种阈值同时使用，一种是召回得分阈值，第二个是召回数量。二者任意一个满足就进行截断。得分阈值通过召回效果来设置的，通过调节阈值，来观察召回的hit和recall等指标，选择了一个指标拐点，即再增大阈值，召回率不明显提升了。

#### &#xA;31.意图分类模块是不是分的类别有点宽泛了？这会不会导致数据前期就分错呢？你这个有测试过吗？

答：有测试过，意图分类模块考虑了领域知识库的内容，因为是封闭领域准确率还是笔记高的，根据测试结果基本准备率再90%+以上。但意图分类有一个难点，就是多轮问答时，分类效果不是很好，容易发生误拒识。这块的优化方向需要结合上下文来判别当前的query是不是车知识的问题。

#### &#xA;32.你做的query改写泛化是不是有点宽泛？

答：query改写尝试过外面开源的通用改写模型，但是这种改写准确率不高，特别是涉及专业知识的问题。采用LLM进行改写，如果大模型足够大，其实改写效果还不错，基本能覆盖常见情况。通过调优prompt，可以让模型尽量不要乱改，保证precision。但因为很多专有名词，领域词汇的引入，部分query改写也有badcase。后续的优化方向是利用专有语料微调一个领域改写模型，模型的参数量选择小一点的，这样可以保证改写的速度，又不会过分影响端到端的延迟。

